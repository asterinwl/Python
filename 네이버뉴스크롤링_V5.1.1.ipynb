{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d6ea3-4148-4fe8-958e-b5e6db33c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pynori.korean_analyzer import KoreanAnalyzer\n",
    "\n",
    "import time\n",
    "import schedule\n",
    "\n",
    "\n",
    "es = Elasticsearch('http://127.0.0.1:9200', timeout=30, max_retries=10, retry_on_timeout=True) # 엘라스틱의 타임아웃 오류를 방지하고자 변수를 더 넣었다.\n",
    " \n",
    "# 키워드는 \"빅데이터\",\"빅데이타\" 연관검색어는 1개만 넣는것이 좋음 (중복된 데이터가 수집됨) \n",
    "competitor=['카카오','삼성sds','lgcns','디리아','메가존클라우드','nshc',\"cloudera\",\"엘라스틱서치코리아\",\"네이버\",\"이지팜\"]\n",
    "bigdata_industry = [\"빅데이터\", \"마이데이터\", \"가트너\",\"DW\", \"데이터웨어하우스\", \"Migration\", \"ETL\",\"EAI\",\"ESB\",\"cloud\",\"AWS\",\"GCP\"]\n",
    "bigdata_solution = [\"splunk\",\"apache kafka\", \"apache spark\", \"elasticsearch\",\"zookeeper\", \"confluent\",\"yarn\",\"kibana\",\"logstash\",\"file beats\",\"docker\",\"kubernetes\"]\n",
    "keyword = [competitor , bigdata_industry , bigdata_solution]\n",
    "\n",
    "now = datetime.now()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = now - timedelta(days=1)\n",
    "yesterday = yesterday.strftime('%Y-%m-%d-%H-%M')\n",
    "\n",
    "title_list = []\n",
    "titlekeyword_list = []\n",
    "url_list = []\n",
    "comp_list = []\n",
    "thumbnail_list = []\n",
    "text_list = []\n",
    "day_list = []\n",
    "keyword_list =[]\n",
    "category_list = []\n",
    "keyword = sum(keyword,[])\n",
    "\n",
    "# python과 elasticsearch의 nori를 합쳐서 만든 새로운 패키지 pynori(2020.07 생겨남), pynori에서 KoreanAnalyzer 패키지 사용\n",
    "# 밑의 코드는 문장에서 '명사'만 골라오는 코드이다.\n",
    "nori = KoreanAnalyzer(\n",
    "           decompound_mode='DISCARD', # DISCARD or MIXED or NONE\n",
    "           infl_decompound_mode='DISCARD', # DISCARD or MIXED or NONE\n",
    "           discard_punctuation=True,\n",
    "           output_unknown_unigrams=False,\n",
    "           pos_filter=True, stop_tags=['JKS',\n",
    "                                       'JKB',\n",
    "                                       'VV',\n",
    "                                       'EF',\n",
    "                                       'EP',\n",
    "                                       'JX',\n",
    "                                       'VA',\n",
    "                                       'ETM',\n",
    "                                       'VCP',\n",
    "                                       \"E\",\n",
    "                                      \"IC\",\n",
    "                                      \"J\",\n",
    "                                      \"MAG\",\n",
    "                                      \"MAJ\",\n",
    "                                      \"MM\",\n",
    "                                      \"NR\",\n",
    "                                      \"NA\",\n",
    "                                      \"SC\",\n",
    "                                      \"SE\",\n",
    "                                      \"SF\",\n",
    "                                      \"SH\",\n",
    "                                      \"SN\",\n",
    "                                      \"SP\",\n",
    "                                      \"SSC\",\n",
    "                                      \"SSO\",\n",
    "                                      \"SY\",\n",
    "                                      \"UNA\",\n",
    "                                      \"UNKNOWN\",\n",
    "                                      \"VA\",\n",
    "                                      \"VCN\",\n",
    "                                      \"VCP\",\n",
    "                                      \"VSV\",\n",
    "                                      \"VV\",\n",
    "                                      \"VX\",\n",
    "                                      \"XPN\",\n",
    "                                      \"XR\",\n",
    "                                      \"XSA\",\n",
    "                                      \"XSN\",\n",
    "                                      \"XSV\",\n",
    "                                      \"NNB\",\n",
    "                                      \"NNBC\",\n",
    "                                      \"EC\",\n",
    "                                      \"JKO\",\n",
    "                                      \"NP\",\n",
    "                                      \"SL\"\n",
    "                                      ],\n",
    "           synonym_filter=False, mode_synonym='NORM', # NORM or EXTENSION\n",
    "       ) \n",
    "\n",
    "def printhello():\n",
    "    \n",
    "    for j in range(len(competitor)+len(bigdata_industry)+len(bigdata_solution)) :\n",
    "\n",
    "\n",
    "        response1 = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query='+ keyword[j]\n",
    "        response2= '&sort=0&photo=0&field=0&pd=4&ds=' + today + '&de=' + yesterday + '&cluster_rank=27&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:1d,a:all&start='\n",
    "\n",
    "        response = requests.get(response1+response2+str(1))\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        articles = soup.select('#main_pack > section > div > div.group_news > ul > li')\n",
    "\n",
    "\n",
    "        for article in articles:\n",
    "            global title_list\n",
    "            \n",
    "            a_tag1 = article.select_one('.news_tit')\n",
    "\n",
    "            title = a_tag1.text\n",
    "            title_list.append(title.strip())\n",
    "            titles = nori.do_analysis(title)['termAtt']  # pynori로 분석해주기 \n",
    "            titlekeyword_list.append(titles)             # 이중 list꼴로 되어 있음 -> 엘라스틱서치 삽입시 문제 없음\n",
    "\n",
    "            url = a_tag1['href']\n",
    "            url_list.append(url.strip())\n",
    "\n",
    "            try:                                                   \n",
    "                comp = article.select_one('a.info.press').text\n",
    "                comp = comp.replace('언론사 선정', '')\n",
    "                comp_list.append(comp.strip())\n",
    "             \n",
    "            except:\n",
    "                comp_list.append('조선일보')\n",
    "            # 일부 기사는 언론사가 없는 경우가 있다. 따라서 언론사가 없는 경우 기본값을 설정해주었다.\n",
    "                \n",
    "            # except Exception as e:\n",
    "                # continue\n",
    "\n",
    "            text = article.select_one('a.api_txt_lines.dsc_txt_wrap').text\n",
    "            text_list.append(text.strip())\n",
    "\n",
    "            day_list.append(today)\n",
    "\n",
    "            keyword_list.append(keyword[j])\n",
    "\n",
    "            try:\n",
    "                thumbnail = article.select_one('div > a > img')['src']\n",
    "                thumbnail_list.append(thumbnail.strip())\n",
    "\n",
    "            except:\n",
    "                thumbnail_list.append(\n",
    "                    'https://search.pstatic.net/common/?src=https%3A%2F%2Fimgnews.pstatic.net%2Fimage%2Forigin%2F366%2F2021%2F09%2F23%2F762207.jpg&type=ff264_180&expire=2&refresh=true')\n",
    "            # 일부 기사는 이미지가 없는 경우가 있다. 따라서 이미지가 없는 경우 기본값을 설정해주었다.\n",
    "\n",
    "\n",
    "            if (keyword[j] in competitor):\n",
    "                category_list.append(\"competitor\")\n",
    "\n",
    "            elif (keyword[j] in bigdata_industry):\n",
    "                category_list.append(\"bigdata_industry\")\n",
    "\n",
    "            elif (keyword[j] in bigdata_solution):\n",
    "                category_list.append(\"bigdata_solution\")\n",
    "                             \n",
    "    info = {'title':title_list, 'titlekeyword':titlekeyword_list ,'url':url_list, 'company':comp_list, 'contents':text_list, 'date':day_list, 'image':thumbnail_list ,'keyword':keyword_list, 'category': category_list}\n",
    "\n",
    "    # print(info)  \n",
    "    \n",
    "    index_name = 'kmh_test4'+\"_\"+today\n",
    "\n",
    "    for i in range(len(info['url'])): \n",
    "        es.index(index=index_name, \n",
    "            doc_type='_doc', \n",
    "            document={'title':info['title'][i],         # body는 업그레이드 버전에서 사라질 예정이며 body대신 document를 사용해라.\n",
    "            'titlekeyword':info['titlekeyword'][i],\n",
    "            'url':info['url'][i], \n",
    "            'company':info['company'][i],\n",
    "            'contents':info['contents'][i], \n",
    "            'date':info['date'][i],\n",
    "            'image':info['image'][i],\n",
    "            'keyword':info['keyword'][i],\n",
    "            'category':info['category'][i]})\n",
    "\n",
    "    es.indices.put_alias(index = index_name, name = 'kmh_test4')\n",
    "    \n",
    "schedule.every(1).minutes.do(printhello) #1분마다 실행\n",
    "schedule.every().monday.at(\"00:10\").do(printhello) #월요일 00:10분에 실행\n",
    "schedule.every().day.at(\"10:30\").do(printhello) #매일 10시 30분에  \n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa2def7-95e4-4861-ba14-e882dcc10172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mongta",
   "language": "python",
   "name": "mongta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
